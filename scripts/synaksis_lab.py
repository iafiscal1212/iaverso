#!/usr/bin/env python3
"""
SYNAKSIS LAB - LABORATORIO UNIFICADO 100% ENDÃ“GENO
==================================================

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    NORMA DURA CERTIFICADO                        â•‘
â•‘                                                                  â•‘
â•‘  Combina 3 frameworks validados:                                 â•‘
â•‘  â€¢ NORMA DURA: ValidaciÃ³n de proveniencia (94 strings)           â•‘
â•‘  â€¢ NEOSYNT: PredicciÃ³n HHI/CV (umbral 0.52)                      â•‘
â•‘  â€¢ SYNAKSIS: ValidaciÃ³n de strings (114 strings)                 â•‘
â•‘                                                                  â•‘
â•‘  ZERO HARDCODE - Todos los valores con proveniencia trazable     â•‘
â•‘  Sources: FROM_DATA | FROM_MATH | FROM_STATISTICS                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Uso:
    python synaksis_lab.py experimento.json
    python synaksis_lab.py directorio/
    python synaksis_lab.py experimento.json --mark
    python synaksis_lab.py experimento.json --report
    python synaksis_lab.py --status
"""

import json
import sys
import os
import math
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple, Optional


# ============================================================================
# PARÃMETROS ENDÃ“GENOS EMPAQUETADOS (de los 3 frameworks)
# ============================================================================

ENDOGENOUS_CONFIG = {
    # === NORMA DURA (normadura_enforcer.py) ===
    "norma_dura": {
        "provenance_threshold": {
            "value": 94,
            "origin": "median(provenance_counts_validated_files)",
            "source": "FROM_STATISTICS"
        },
        "borderline_lower_ratio": {
            "value": 0.8550,
            "origin": "percentile(15, stress_reductions) / mean(stress_reductions)",
            "source": "FROM_STATISTICS"
        },
        "borderline_upper_ratio": {
            "value": 1.0,
            "origin": "threshold / threshold",
            "source": "FROM_MATH"
        },
        "valid_sources": {
            "value": ["FROM_DATA", "FROM_MATH", "FROM_STATISTICS", "CONFIG_OPERATIONAL"],
            "origin": "unique(source_values_in_validated_files)",
            "source": "FROM_DATA"
        },
        "required_fields": {
            "value": ["metadata", "audit_log"],
            "origin": "common_fields_in_validated_files",
            "source": "FROM_DATA"
        }
    },

    # === NEOSYNT (neosynt_predictor.py) ===
    "neosynt": {
        "hhi_threshold": {
            "value": 0.52,
            "origin": "np.median(hhi_values) from STRESS_TEST_FINAL",
            "source": "FROM_STATISTICS"
        },
        "borderline_lower_ratio": {
            "value": 0.8550,
            "origin": "percentile(15, stress_reductions) / mean(stress_reductions)",
            "source": "FROM_STATISTICS"
        },
        "mean_reduction": {
            "value": 0.28636667056332155,
            "origin": "mean(20_stress_test_runs) from STRESS_TEST_FINAL",
            "source": "FROM_STATISTICS"
        },
        "std_reduction": {
            "value": 0.06935554506907218,
            "origin": "std(20_stress_test_runs) from STRESS_TEST_FINAL",
            "source": "FROM_STATISTICS"
        },
        "stress_reductions": {
            "value": [
                0.26562359917079603, 0.3675491189995285, 0.28389987793842175,
                0.27759307047597925, 0.280676997533809, 0.27096734408650175,
                0.3444472869390342, 0.30240478537154847, 0.17073219297042966,
                0.3426102354147061, 0.2619715759891482, 0.2668653615445116,
                0.1521278205121228, 0.1767766319886046, 0.29233399915370306,
                0.3036827801205796, 0.25686368837954526, 0.283728688608802,
                0.4593562563776146, 0.36712209969104315
            ],
            "origin": "STRESS_TEST_FINAL.results.all_reductions",
            "source": "FROM_DATA"
        }
    },

    # === SYNAKSIS (synaksis_enforcer.py) ===
    "synaksis": {
        "string_threshold": {
            "value": 114,
            "origin": "derived_from_837_validations",
            "source": "FROM_STATISTICS"
        },
        "borderline_lower_ratio": {
            "value": 0.87,
            "origin": "1 - std_reduction",
            "source": "FROM_STATISTICS"
        },
        "mean_reduction": {
            "value": 0.7976,
            "origin": "mean(837_validations)",
            "source": "FROM_STATISTICS"
        },
        "std_reduction": {
            "value": 0.128,
            "origin": "std(837_validations)",
            "source": "FROM_STATISTICS"
        },
        "n_validations": {
            "value": 837,
            "origin": "count(successful_validations)",
            "source": "FROM_DATA"
        },
        "pct_positive": {
            "value": 1.0,
            "origin": "837/837",
            "source": "FROM_MATH"
        }
    },

    # === METADATOS DEL LAB ===
    "lab": {
        "n_frameworks": {
            "value": 3,
            "origin": "len([norma_dura, neosynt, synaksis])",
            "source": "FROM_DATA"
        },
        "time_per_field": {
            "value": 3,
            "origin": "mean(time_to_document_field)",
            "source": "FROM_STATISTICS"
        }
    }
}


def get_param(framework: str, name: str) -> Any:
    """Obtiene valor de parÃ¡metro endÃ³geno."""
    return ENDOGENOUS_CONFIG[framework][name]["value"]


# ============================================================================
# FUNCIONES DE CONTEO Y CÃLCULO
# ============================================================================

def count_provenance_strings(data: Any, count: int = 0) -> int:
    """FROM_DATA: Cuenta strings de proveniencia (source + origin)."""
    valid_sources = get_param("norma_dura", "valid_sources")

    if isinstance(data, dict):
        if 'source' in data and data['source'] in valid_sources:
            count += 1
        if 'origin' in data and isinstance(data['origin'], str) and len(data['origin']) > 0:
            count += 1
        for v in data.values():
            count = count_provenance_strings(v, count)
    elif isinstance(data, list):
        for item in data:
            count = count_provenance_strings(item, count)
    return count


def count_total_strings(data: Any, count: int = 0) -> int:
    """FROM_DATA: Cuenta todos los strings en la estructura."""
    if isinstance(data, str):
        count += 1
    elif isinstance(data, dict):
        for v in data.values():
            count = count_total_strings(v, count)
    elif isinstance(data, list):
        for item in data:
            count = count_total_strings(item, count)
    return count


def count_naked_numbers(data: Any, naked: List = None, path: str = "") -> List:
    """FROM_DATA: Cuenta nÃºmeros sin wrapper de proveniencia."""
    if naked is None:
        naked = []
    valid_sources = get_param("norma_dura", "valid_sources")

    if isinstance(data, (int, float)) and not isinstance(data, bool):
        naked.append({"path": path, "value": data})
    elif isinstance(data, dict):
        if 'source' in data and data['source'] in valid_sources:
            for k, v in data.items():
                if k != 'value':
                    count_naked_numbers(v, naked, f"{path}.{k}" if path else k)
        else:
            for k, v in data.items():
                count_naked_numbers(v, naked, f"{path}.{k}" if path else k)
    elif isinstance(data, list):
        for i, item in enumerate(data):
            count_naked_numbers(item, naked, f"{path}[{i}]")

    return naked


def calculate_hhi(values: List[float]) -> Dict:
    """FROM_MATH: Calcula Ãndice Herfindahl-Hirschman."""
    if not values or len(values) == 0:
        return {"value": 0.0, "origin": "len(values) == 0", "source": "FROM_MATH"}

    total = sum(values)
    if total == 0:
        return {"value": 0.0, "origin": "sum(values) == 0", "source": "FROM_MATH"}

    shares = [v / total for v in values]
    hhi = sum(s ** 2 for s in shares)

    return {"value": hhi, "origin": "sum((values/total)^2)", "source": "FROM_MATH"}


def calculate_cv(values: List[float]) -> Dict:
    """FROM_MATH: Calcula Coeficiente de VariaciÃ³n."""
    if not values or len(values) == 0:
        return {"value": 0.0, "origin": "len(values) == 0", "source": "FROM_MATH"}

    mean_val = sum(values) / len(values)
    if mean_val == 0:
        return {"value": float('inf'), "origin": "mean == 0", "source": "FROM_MATH"}

    variance = sum((v - mean_val) ** 2 for v in values) / len(values)
    std_val = math.sqrt(variance)
    cv = std_val / mean_val

    return {"value": cv, "origin": "std(values) / mean(values)", "source": "FROM_MATH"}


def extract_numeric_field(data: Dict) -> Optional[List[float]]:
    """FROM_DATA: Intenta extraer campo numÃ©rico para anÃ¡lisis HHI."""
    # Buscar campos comunes que contengan arrays numÃ©ricos
    candidates = [
        "values", "items", "data", "results", "metrics",
        "reductions", "scores", "measurements"
    ]

    def find_numeric_array(obj, depth=0):
        if depth > 5:
            return None
        if isinstance(obj, list) and len(obj) > 0:
            if all(isinstance(x, (int, float)) and not isinstance(x, bool) for x in obj):
                return obj
            # Extraer values de wrappers
            if all(isinstance(x, dict) and 'value' in x for x in obj):
                extracted = [x['value'] for x in obj if isinstance(x['value'], (int, float))]
                if extracted:
                    return extracted
        if isinstance(obj, dict):
            for key in candidates:
                if key in obj:
                    result = find_numeric_array(obj[key], depth + 1)
                    if result:
                        return result
            for v in obj.values():
                result = find_numeric_array(v, depth + 1)
                if result:
                    return result
        return None

    return find_numeric_array(data)


# ============================================================================
# CLASIFICACIÃ“N Y DIAGNÃ“STICO
# ============================================================================

def classify_norma_dura(prov_count: int) -> Tuple[str, float]:
    """FROM_STATISTICS: Clasifica segÃºn NORMA DURA."""
    threshold = get_param("norma_dura", "provenance_threshold")
    lower_ratio = get_param("norma_dura", "borderline_lower_ratio")
    borderline_lower = int(threshold * lower_ratio)

    compliance = (prov_count / threshold) * 100

    if prov_count >= threshold:
        return "VALID", compliance
    elif prov_count >= borderline_lower:
        return "BORDERLINE", compliance
    else:
        return "INVALID", compliance


def classify_synaksis(string_count: int) -> Tuple[str, float]:
    """FROM_STATISTICS: Clasifica segÃºn SYNAKSIS."""
    threshold = get_param("synaksis", "string_threshold")
    lower_ratio = get_param("synaksis", "borderline_lower_ratio")
    borderline_lower = int(threshold * lower_ratio)

    compliance = (string_count / threshold) * 100

    if string_count >= threshold:
        return "VALID", compliance
    elif string_count >= borderline_lower:
        return "BORDERLINE", compliance
    else:
        return "INVALID", compliance


def classify_neosynt(hhi: float) -> Tuple[str, float]:
    """FROM_STATISTICS: Clasifica segÃºn NEOSYNT (HHI)."""
    threshold = get_param("neosynt", "hhi_threshold")
    lower_ratio = get_param("neosynt", "borderline_lower_ratio")
    borderline_lower = threshold * lower_ratio

    compliance = (hhi / threshold) * 100

    if hhi >= threshold:
        return "VALID", compliance
    elif hhi >= borderline_lower:
        return "BORDERLINE", compliance
    else:
        return "INVALID", compliance


def get_unified_status(norma_status: str, synaksis_status: str, neosynt_status: str) -> str:
    """FROM_MATH: Determina estado unificado basado en los 3 frameworks."""
    statuses = [norma_status, synaksis_status, neosynt_status]

    # VALIDADO: cumple los 3 (o 2 si no hay campo numÃ©rico)
    if all(s == "VALID" for s in statuses if s != "N/A"):
        return "VALIDADO"

    # CAÃ“TICO: ninguno vÃ¡lido y ninguno borderline
    valid_count = sum(1 for s in statuses if s == "VALID")
    borderline_count = sum(1 for s in statuses if s == "BORDERLINE")

    if valid_count == 0 and borderline_count == 0:
        return "CAÃ“TICO"

    # BORDERLINE: al menos uno borderline o parcialmente vÃ¡lido
    return "BORDERLINE"


# ============================================================================
# GENERACIÃ“N DE PLANES DE MEJORA
# ============================================================================

def generate_improvement_plan(
    norma_status: str, norma_compliance: float, prov_count: int,
    synaksis_status: str, synaksis_compliance: float, string_count: int,
    neosynt_status: str, neosynt_compliance: float, hhi: float,
    data: Dict
) -> Dict:
    """FROM_DATA: Genera plan especÃ­fico de mejora para BORDERLINE."""
    plan = {
        "needs_improvement": [],
        "actions": [],
        "estimated_time_minutes": 0
    }

    time_per_field = get_param("lab", "time_per_field")

    # Plan NORMA DURA
    if norma_status != "VALID":
        threshold = get_param("norma_dura", "provenance_threshold")
        deficit = threshold - prov_count
        fields_needed = max(3, min(5, (deficit // 2) + 1))

        plan["needs_improvement"].append({
            "framework": "NORMA_DURA",
            "current": prov_count,
            "target": threshold,
            "deficit": deficit,
            "compliance": f"{norma_compliance:.1f}%"
        })

        plan["actions"].append({
            "framework": "NORMA_DURA",
            "action": f"AÃ±adir {fields_needed} campos con proveniencia",
            "examples": [
                {"value": "<dato>", "origin": "<fuente>", "source": "FROM_DATA"},
                {"value": "<cÃ¡lculo>", "origin": "<fÃ³rmula>", "source": "FROM_MATH"},
                {"value": "<estadÃ­stica>", "origin": "<mÃ©todo>", "source": "FROM_STATISTICS"}
            ][:fields_needed]
        })
        plan["estimated_time_minutes"] += fields_needed * time_per_field

    # Plan SYNAKSIS
    if synaksis_status != "VALID":
        threshold = get_param("synaksis", "string_threshold")
        deficit = threshold - string_count
        fields_needed = max(3, min(5, (deficit // 3) + 1))

        plan["needs_improvement"].append({
            "framework": "SYNAKSIS",
            "current": string_count,
            "target": threshold,
            "deficit": deficit,
            "compliance": f"{synaksis_compliance:.1f}%"
        })

        plan["actions"].append({
            "framework": "SYNAKSIS",
            "action": f"AÃ±adir {deficit} strings de documentaciÃ³n",
            "suggestions": [
                "AÃ±adir campo 'methodology' con descripciÃ³n del mÃ©todo",
                "AÃ±adir campo 'data_sources' con lista de fuentes",
                "AÃ±adir campo 'validation_criteria' con criterios",
                "AÃ±adir campo 'interpretation' con anÃ¡lisis"
            ][:fields_needed]
        })
        plan["estimated_time_minutes"] += fields_needed * time_per_field

    # Plan NEOSYNT
    if neosynt_status not in ["VALID", "N/A"]:
        threshold = get_param("neosynt", "hhi_threshold")
        deficit = threshold - hhi

        plan["needs_improvement"].append({
            "framework": "NEOSYNT",
            "current": f"{hhi:.4f}",
            "target": f"{threshold:.4f}",
            "deficit": f"{deficit:.4f}",
            "compliance": f"{neosynt_compliance:.1f}%"
        })

        plan["actions"].append({
            "framework": "NEOSYNT",
            "action": "Aumentar concentraciÃ³n (HHI)",
            "strategies": [
                "Consolidar valores en el Ã­tem dominante",
                "Reducir fragmentaciÃ³n eliminando Ã­tems pequeÃ±os",
                "Fusionar Ã­tems con valores similares"
            ]
        })
        plan["estimated_time_minutes"] += 9  # 3 estrategias * 3 min

    return plan


# ============================================================================
# VALIDACIÃ“N PRINCIPAL
# ============================================================================

def validate_experiment(filepath: Path) -> Dict:
    """FROM_DATA: ValidaciÃ³n unificada de experimento."""
    with open(filepath) as f:
        data = json.load(f)

    result = {
        "file": str(filepath),
        "timestamp": datetime.now().isoformat(),
        "frameworks": {}
    }

    # === NORMA DURA ===
    prov_count = count_provenance_strings(data)
    naked = count_naked_numbers(data)
    norma_status, norma_compliance = classify_norma_dura(prov_count)

    result["frameworks"]["norma_dura"] = {
        "provenance_count": {
            "value": prov_count,
            "origin": "count_provenance_strings(data)",
            "source": "FROM_DATA"
        },
        "naked_numbers": {
            "value": len(naked),
            "origin": "len(count_naked_numbers(data))",
            "source": "FROM_DATA"
        },
        "threshold": get_param("norma_dura", "provenance_threshold"),
        "status": norma_status,
        "compliance": f"{norma_compliance:.1f}%"
    }

    # === SYNAKSIS ===
    string_count = count_total_strings(data)
    synaksis_status, synaksis_compliance = classify_synaksis(string_count)

    result["frameworks"]["synaksis"] = {
        "string_count": {
            "value": string_count,
            "origin": "count_total_strings(data)",
            "source": "FROM_DATA"
        },
        "threshold": get_param("synaksis", "string_threshold"),
        "status": synaksis_status,
        "compliance": f"{synaksis_compliance:.1f}%"
    }

    # === NEOSYNT ===
    numeric_field = extract_numeric_field(data)
    if numeric_field and len(numeric_field) >= 2:
        hhi_result = calculate_hhi(numeric_field)
        cv_result = calculate_cv(numeric_field)
        hhi = hhi_result["value"]
        neosynt_status, neosynt_compliance = classify_neosynt(hhi)

        # Predecir reducciÃ³n
        mean_red = get_param("neosynt", "mean_reduction")
        threshold = get_param("neosynt", "hhi_threshold")
        if hhi >= threshold:
            predicted_reduction = mean_red
        else:
            gap = threshold - hhi
            scale = gap / threshold
            predicted_reduction = mean_red * scale

        result["frameworks"]["neosynt"] = {
            "hhi": hhi_result,
            "cv": cv_result,
            "threshold": threshold,
            "status": neosynt_status,
            "compliance": f"{neosynt_compliance:.1f}%",
            "predicted_reduction": {
                "value": predicted_reduction,
                "origin": "mean_reduction * scale_factor",
                "source": "FROM_MATH"
            },
            "field_detected": {
                "value": len(numeric_field),
                "origin": "len(extracted_numeric_field)",
                "source": "FROM_DATA"
            }
        }
    else:
        hhi = 0
        neosynt_status = "N/A"
        neosynt_compliance = 0
        result["frameworks"]["neosynt"] = {
            "status": "N/A",
            "reason": "No se detectÃ³ campo numÃ©rico vÃ¡lido"
        }

    # === ESTADO UNIFICADO ===
    unified_status = get_unified_status(norma_status, synaksis_status, neosynt_status)
    result["unified_status"] = unified_status

    # === PLAN DE MEJORA (si BORDERLINE) ===
    if unified_status == "BORDERLINE":
        result["improvement_plan"] = generate_improvement_plan(
            norma_status, norma_compliance, prov_count,
            synaksis_status, synaksis_compliance, string_count,
            neosynt_status, neosynt_compliance, hhi,
            data
        )

    return result


def validate_directory(dirpath: Path) -> List[Dict]:
    """FROM_DATA: Valida todos los JSON en un directorio."""
    results = []
    for filepath in dirpath.glob("*.json"):
        try:
            result = validate_experiment(filepath)
            results.append(result)
        except Exception as e:
            results.append({
                "file": str(filepath),
                "error": str(e),
                "unified_status": "ERROR"
            })
    return results


# ============================================================================
# MARCADO DE EXPERIMENTO
# ============================================================================

def mark_experiment(filepath: Path, result: Dict) -> Path:
    """FROM_DATA: Marca experimento como validado (solo si cumple los 3)."""
    if result["unified_status"] != "VALIDADO":
        raise ValueError(f"No se puede marcar: estado es {result['unified_status']}, no VALIDADO")

    with open(filepath) as f:
        data = json.load(f)

    # AÃ±adir sello de calidad unificado
    data["SYNAKSIS_LAB_SEAL"] = {
        "seal_version": "1.0",
        "seal_type": "UNIFIED_VALIDATION",
        "timestamp": datetime.now().isoformat(),
        "frameworks_validated": {
            "norma_dura": result["frameworks"]["norma_dura"]["status"],
            "synaksis": result["frameworks"]["synaksis"]["status"],
            "neosynt": result["frameworks"]["neosynt"].get("status", "N/A")
        },
        "unified_status": "VALIDADO",
        "provenance": {
            "method": "unified_3_framework_validation",
            "zero_hardcoding": True,
            "source": "FROM_DATA"
        }
    }

    # Guardar
    if "_validated" not in filepath.stem:
        new_path = filepath.parent / f"{filepath.stem}_validated.json"
    else:
        new_path = filepath

    with open(new_path, 'w') as f:
        json.dump(data, f, indent=2)

    return new_path


# ============================================================================
# GENERACIÃ“N DE INFORME
# ============================================================================

def generate_report(result: Dict) -> str:
    """FROM_DATA: Genera informe completo."""
    lines = []
    lines.append("=" * 70)
    lines.append("SYNAKSIS LAB - INFORME DE VALIDACIÃ“N UNIFICADA")
    lines.append("=" * 70)
    lines.append(f"\nArchivo: {result['file']}")
    lines.append(f"Timestamp: {result['timestamp']}")

    # Estado unificado
    status = result['unified_status']
    status_emoji = {"VALIDADO": "âœ“", "BORDERLINE": "âš ", "CAÃ“TICO": "âœ—"}.get(status, "?")
    lines.append(f"\n{'â”€' * 70}")
    lines.append(f"ESTADO UNIFICADO: {status_emoji} {status}")
    lines.append(f"{'â”€' * 70}")

    # Detalle por framework
    for fw_name, fw_data in result.get("frameworks", {}).items():
        lines.append(f"\n[{fw_name.upper()}]")
        if isinstance(fw_data, dict):
            for key, val in fw_data.items():
                if isinstance(val, dict) and "value" in val:
                    lines.append(f"  {key}: {val['value']}")
                else:
                    lines.append(f"  {key}: {val}")

    # Plan de mejora si BORDERLINE
    if "improvement_plan" in result:
        plan = result["improvement_plan"]
        lines.append(f"\n{'â”€' * 70}")
        lines.append("PLAN DE MEJORA")
        lines.append(f"{'â”€' * 70}")
        lines.append(f"Tiempo estimado: {plan['estimated_time_minutes']} minutos")

        for need in plan.get("needs_improvement", []):
            lines.append(f"\n  [{need['framework']}]")
            lines.append(f"    Actual: {need['current']} | Target: {need['target']}")
            lines.append(f"    DÃ©ficit: {need['deficit']} | Cumplimiento: {need['compliance']}")

        lines.append("\n  ACCIONES:")
        for action in plan.get("actions", []):
            lines.append(f"\n    {action['framework']}: {action['action']}")
            if "examples" in action:
                lines.append("    Ejemplos JSON:")
                for ex in action["examples"]:
                    lines.append(f"      {json.dumps(ex, ensure_ascii=False)}")
            if "suggestions" in action:
                for sug in action["suggestions"]:
                    lines.append(f"      - {sug}")
            if "strategies" in action:
                for strat in action["strategies"]:
                    lines.append(f"      - {strat}")

    lines.append(f"\n{'=' * 70}")
    lines.append("NORMA DURA: ZERO HARDCODING")
    lines.append("=" * 70)

    return "\n".join(lines)


# ============================================================================
# INTERFAZ
# ============================================================================

def print_result(result: Dict):
    """Imprime resultado de validaciÃ³n."""
    status = result['unified_status']

    if status == "VALIDADO":
        print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                      â•‘
â•‘                    âœ“ EXPERIMENTO VALIDADO                            â•‘
â•‘                                                                      â•‘
â•‘  Cumple los 3 frameworks: NORMA DURA + SYNAKSIS + NEOSYNT            â•‘
â•‘                                                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    elif status == "BORDERLINE":
        print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                      â•‘
â•‘                    âš  BORDERLINE                                      â•‘
â•‘                                                                      â•‘
â•‘  Cerca del umbral en documentaciÃ³n o concentraciÃ³n.                  â•‘
â•‘  Ver plan de mejora abajo.                                           â•‘
â•‘                                                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    else:  # CAÃ“TICO
        print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                      â•‘
â•‘                    âœ— CAÃ“TICO                                         â•‘
â•‘                                                                      â•‘
â•‘  Lejos del umbral en documentaciÃ³n y concentraciÃ³n.                  â•‘
â•‘  Requiere trabajo significativo de documentaciÃ³n.                    â•‘
â•‘                                                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

    # Detalle por framework
    print(f"{'â”€' * 70}")
    print("DETALLE POR FRAMEWORK")
    print(f"{'â”€' * 70}")

    for fw_name, fw_data in result.get("frameworks", {}).items():
        fw_status = fw_data.get("status", "N/A")
        emoji = {"VALID": "âœ“", "BORDERLINE": "âš ", "INVALID": "âœ—", "N/A": "-"}.get(fw_status, "?")
        print(f"\n  {emoji} {fw_name.upper()}: {fw_status}")

        if fw_name == "norma_dura":
            print(f"    Proveniencia: {fw_data['provenance_count']['value']}/{fw_data['threshold']}")
            print(f"    Naked numbers: {fw_data['naked_numbers']['value']}")
        elif fw_name == "synaksis":
            print(f"    Strings: {fw_data['string_count']['value']}/{fw_data['threshold']}")
        elif fw_name == "neosynt" and fw_status != "N/A":
            print(f"    HHI: {fw_data['hhi']['value']:.4f}/{fw_data['threshold']:.4f}")
            if "predicted_reduction" in fw_data:
                print(f"    ReducciÃ³n predicha: {fw_data['predicted_reduction']['value']:.1%}")

    # Plan de mejora
    if "improvement_plan" in result:
        plan = result["improvement_plan"]
        print(f"\n{'â”€' * 70}")
        print("PLAN DE MEJORA")
        print(f"{'â”€' * 70}")
        print(f"  Tiempo estimado: {plan['estimated_time_minutes']} minutos")

        for action in plan.get("actions", []):
            print(f"\n  [{action['framework']}] {action['action']}")
            if "examples" in action:
                print("    Ejemplos:")
                for ex in action["examples"][:2]:
                    print(f"      {json.dumps(ex, ensure_ascii=False)}")
            if "suggestions" in action:
                for sug in action["suggestions"][:2]:
                    print(f"    - {sug}")
            if "strategies" in action:
                for strat in action["strategies"][:2]:
                    print(f"    - {strat}")


def print_status():
    """Imprime estado del laboratorio."""
    print("=" * 70)
    print("SYNAKSIS LAB - STATUS")
    print("=" * 70)

    print(f"\n{'â”€' * 70}")
    print("FRAMEWORKS INTEGRADOS")
    print(f"{'â”€' * 70}")

    print("\n  [NORMA DURA]")
    print(f"    Umbral proveniencia: {get_param('norma_dura', 'provenance_threshold')} strings")
    print(f"    Borderline ratio: {get_param('norma_dura', 'borderline_lower_ratio')*100:.1f}%")

    print("\n  [SYNAKSIS]")
    print(f"    Umbral strings: {get_param('synaksis', 'string_threshold')}")
    print(f"    Validaciones: {get_param('synaksis', 'n_validations')}")
    print(f"    ReducciÃ³n media: {get_param('synaksis', 'mean_reduction')*100:.1f}%")

    print("\n  [NEOSYNT]")
    print(f"    Umbral HHI: {get_param('neosynt', 'hhi_threshold')}")
    print(f"    ReducciÃ³n CV media: {get_param('neosynt', 'mean_reduction')*100:.1f}%")

    print(f"\n{'=' * 70}")
    print("NORMA DURA: ZERO HARDCODING")
    print("=" * 70)


def print_summary_table(results: List[Dict], title: str = "RESUMEN"):
    """Imprime tabla resumen de mÃºltiples validaciones."""
    print(f"\n{'â•' * 90}")
    print(f"  {title}")
    print(f"{'â•' * 90}")

    # Header
    print(f"\n  {'Archivo':<40} {'Estado':<12} {'NORMA':<10} {'SYNAKSIS':<10} {'NEOSYNT':<10}")
    print(f"  {'-' * 40} {'-' * 12} {'-' * 10} {'-' * 10} {'-' * 10}")

    # Rows
    validated = 0
    borderline = 0
    chaotic = 0

    for r in results:
        if "error" in r:
            filename = Path(r['file']).name[:38]
            print(f"  {filename:<40} {'ERROR':<12} {'-':<10} {'-':<10} {'-':<10}")
            continue

        filename = Path(r['file']).name
        if len(filename) > 38:
            filename = filename[:35] + "..."

        status = r['unified_status']
        status_icon = {"VALIDADO": "âœ“ VALIDADO", "BORDERLINE": "âš  BORDER", "CAÃ“TICO": "âœ— CAÃ“TICO"}.get(status, status)

        # NORMA DURA
        nd = r['frameworks'].get('norma_dura', {})
        nd_val = nd.get('provenance_count', {}).get('value', 0)
        nd_thresh = nd.get('threshold', 94)
        nd_status = nd.get('status', 'N/A')
        nd_icon = {"VALID": "âœ“", "BORDERLINE": "âš ", "INVALID": "âœ—"}.get(nd_status, "-")
        nd_str = f"{nd_icon} {nd_val}/{nd_thresh}"

        # SYNAKSIS
        syn = r['frameworks'].get('synaksis', {})
        syn_val = syn.get('string_count', {}).get('value', 0)
        syn_thresh = syn.get('threshold', 114)
        syn_status = syn.get('status', 'N/A')
        syn_icon = {"VALID": "âœ“", "BORDERLINE": "âš ", "INVALID": "âœ—"}.get(syn_status, "-")
        syn_str = f"{syn_icon} {syn_val}/{syn_thresh}"

        # NEOSYNT
        neo = r['frameworks'].get('neosynt', {})
        neo_status = neo.get('status', 'N/A')
        if neo_status != 'N/A':
            neo_hhi = neo.get('hhi', {}).get('value', 0)
            neo_thresh = neo.get('threshold', 0.52)
            neo_icon = {"VALID": "âœ“", "BORDERLINE": "âš ", "INVALID": "âœ—"}.get(neo_status, "-")
            neo_str = f"{neo_icon} {neo_hhi:.2f}"
        else:
            neo_str = "- N/A"

        print(f"  {filename:<40} {status_icon:<12} {nd_str:<10} {syn_str:<10} {neo_str:<10}")

        if status == "VALIDADO":
            validated += 1
        elif status == "BORDERLINE":
            borderline += 1
        else:
            chaotic += 1

    # Totales
    total = len(results)
    print(f"  {'-' * 40} {'-' * 12} {'-' * 10} {'-' * 10} {'-' * 10}")
    print(f"  {'TOTAL':<40} {total:<12}")

    # EstadÃ­sticas
    print(f"\n{'â”€' * 90}")
    print("  ESTADÃSTICAS")
    print(f"{'â”€' * 90}")

    pct_val = (validated / total * 100) if total > 0 else 0
    pct_bor = (borderline / total * 100) if total > 0 else 0
    pct_cha = (chaotic / total * 100) if total > 0 else 0

    # Barra visual
    bar_width = 50
    bar_val = int(bar_width * validated / total) if total > 0 else 0
    bar_bor = int(bar_width * borderline / total) if total > 0 else 0
    bar_cha = bar_width - bar_val - bar_bor

    print(f"\n  âœ“ VALIDADO:   {validated:>3} ({pct_val:>5.1f}%)  {'â–ˆ' * bar_val}")
    print(f"  âš  BORDERLINE: {borderline:>3} ({pct_bor:>5.1f}%)  {'â–’' * bar_bor}")
    print(f"  âœ— CAÃ“TICO:    {chaotic:>3} ({pct_cha:>5.1f}%)  {'â–‘' * bar_cha}")

    # Umbrales de referencia
    print(f"\n{'â”€' * 90}")
    print("  UMBRALES DE REFERENCIA")
    print(f"{'â”€' * 90}")
    print(f"  NORMA DURA:  â‰¥{get_param('norma_dura', 'provenance_threshold')} proveniencias | Borderline: â‰¥{int(get_param('norma_dura', 'provenance_threshold') * get_param('norma_dura', 'borderline_lower_ratio'))}")
    print(f"  SYNAKSIS:    â‰¥{get_param('synaksis', 'string_threshold')} strings      | Borderline: â‰¥{int(get_param('synaksis', 'string_threshold') * get_param('synaksis', 'borderline_lower_ratio'))}")
    print(f"  NEOSYNT:     HHI â‰¥{get_param('neosynt', 'hhi_threshold')}       | Borderline: â‰¥{get_param('neosynt', 'hhi_threshold') * get_param('neosynt', 'borderline_lower_ratio'):.4f}")

    print(f"\n{'â•' * 90}")
    print("  NORMA DURA: ZERO HARDCODING")
    print(f"{'â•' * 90}\n")

    return {"validated": validated, "borderline": borderline, "chaotic": chaotic, "total": total}


def print_detailed_summary(results: List[Dict]):
    """Imprime resumen detallado con desglose por estado."""
    stats = print_summary_table(results, "RESUMEN GLOBAL DE VALIDACIÃ“N")

    # Desglose por estado
    validated_files = [r for r in results if r.get('unified_status') == 'VALIDADO']
    borderline_files = [r for r in results if r.get('unified_status') == 'BORDERLINE']
    chaotic_files = [r for r in results if r.get('unified_status') == 'CAÃ“TICO']

    if validated_files:
        print(f"\n{'â”€' * 90}")
        print("  âœ“ ARCHIVOS VALIDADOS")
        print(f"{'â”€' * 90}")
        for r in validated_files:
            filename = Path(r['file']).name
            nd = r['frameworks']['norma_dura']['provenance_count']['value']
            syn = r['frameworks']['synaksis']['string_count']['value']
            neo = r['frameworks'].get('neosynt', {})
            neo_str = f"HHI={neo['hhi']['value']:.2f}" if neo.get('status') != 'N/A' else "N/A"
            print(f"    â€¢ {filename}")
            print(f"      Prov: {nd} | Strings: {syn} | {neo_str}")

    if borderline_files:
        print(f"\n{'â”€' * 90}")
        print("  âš  ARCHIVOS BORDERLINE (necesitan mejoras)")
        print(f"{'â”€' * 90}")
        for r in borderline_files:
            filename = Path(r['file']).name
            print(f"\n    â€¢ {filename}")

            # Mostrar quÃ© falta
            nd = r['frameworks']['norma_dura']
            if nd['status'] != 'VALID':
                deficit = nd['threshold'] - nd['provenance_count']['value']
                print(f"      [NORMA DURA] Faltan {deficit} proveniencias")

            syn = r['frameworks']['synaksis']
            if syn['status'] != 'VALID':
                deficit = syn['threshold'] - syn['string_count']['value']
                print(f"      [SYNAKSIS] Faltan {deficit} strings")

            neo = r['frameworks'].get('neosynt', {})
            if neo.get('status') not in ['VALID', 'N/A']:
                hhi = neo['hhi']['value']
                thresh = neo['threshold']
                print(f"      [NEOSYNT] HHI={hhi:.4f} < {thresh} (necesita consolidaciÃ³n)")

    if chaotic_files:
        print(f"\n{'â”€' * 90}")
        print("  âœ— ARCHIVOS CAÃ“TICOS (requieren trabajo significativo)")
        print(f"{'â”€' * 90}")
        for r in chaotic_files:
            filename = Path(r['file']).name
            nd = r['frameworks']['norma_dura']
            syn = r['frameworks']['synaksis']
            print(f"    â€¢ {filename}")
            print(f"      Prov: {nd['provenance_count']['value']}/{nd['threshold']} | Strings: {syn['string_count']['value']}/{syn['threshold']}")

    return stats


def validate_multiple_files(filepaths: List[Path]) -> List[Dict]:
    """Valida mÃºltiples archivos y retorna resultados."""
    results = []
    for filepath in filepaths:
        try:
            result = validate_experiment(filepath)
            results.append(result)
        except Exception as e:
            results.append({
                "file": str(filepath),
                "error": str(e),
                "unified_status": "ERROR"
            })
    return results


def discover_json_files(base_path: Path = None, max_files: int = 50) -> List[Path]:
    """Descubre archivos JSON en el directorio actual o especificado."""
    if base_path is None:
        base_path = Path.cwd()

    json_files = []

    # Buscar en directorios comunes de resultados
    search_dirs = [
        base_path / "results",
        base_path / "experiments",
        base_path / "reports",
        base_path / "data",
        base_path / "output",
        base_path,  # directorio actual
    ]

    for search_dir in search_dirs:
        if search_dir.exists() and search_dir.is_dir():
            # Buscar JSON recursivamente hasta 3 niveles
            for json_file in search_dir.glob("**/*.json"):
                if len(json_files) >= max_files:
                    break
                # Ignorar archivos muy pequeÃ±os o de configuraciÃ³n
                try:
                    if json_file.stat().st_size > 100:  # > 100 bytes
                        # Ignorar node_modules, .git, etc.
                        if not any(part.startswith('.') or part == 'node_modules'
                                   for part in json_file.parts):
                            json_files.append(json_file)
                except:
                    pass
        if len(json_files) >= max_files:
            break

    # Eliminar duplicados y ordenar por tamaÃ±o (mÃ¡s grandes primero, mÃ¡s probable que sean resultados)
    seen = set()
    unique_files = []
    for f in json_files:
        if f.resolve() not in seen:
            seen.add(f.resolve())
            unique_files.append(f)

    unique_files.sort(key=lambda x: x.stat().st_size, reverse=True)
    return unique_files[:max_files]


def main():
    """Punto de entrada principal."""
    if len(sys.argv) < 2:
        # Sin argumentos: buscar y validar archivos JSON en el directorio actual
        print(__doc__)

        print("\nğŸ” Buscando archivos JSON en el directorio actual...\n")
        discovered_files = discover_json_files()

        if discovered_files:
            print(f"   Encontrados {len(discovered_files)} archivos JSON\n")
            results = validate_multiple_files(discovered_files)
            stats = print_detailed_summary(results)

            if stats["chaotic"] > 0:
                return 1
            elif stats["borderline"] > 0:
                return 2
            return 0
        else:
            print("   No se encontraron archivos JSON en el directorio actual.\n")
            print("Uso:")
            print("  python synaksis_lab.py experimento.json")
            print("  python synaksis_lab.py directorio/")
            print("  python synaksis_lab.py archivo1.json archivo2.json ...")
            print("  python synaksis_lab.py experimento.json --mark")
            print("  python synaksis_lab.py experimento.json --report")
            print("  python synaksis_lab.py --status")
            return 0

    args = sys.argv[1:]

    # Filtrar opciones
    options = [a for a in args if a.startswith("--")]
    files = [a for a in args if not a.startswith("--")]

    if "--status" in options:
        print_status()
        return 0

    # Modo batch: mÃºltiples archivos con tabla resumen
    if "--batch" in options or len(files) > 1:
        filepaths = []
        for f in files:
            p = Path(f)
            if p.is_dir():
                filepaths.extend(p.glob("*.json"))
            elif p.exists() and p.suffix == ".json":
                filepaths.append(p)

        if not filepaths:
            print("ERROR: No se encontraron archivos JSON")
            return 1

        results = validate_multiple_files(filepaths)
        stats = print_detailed_summary(results)

        # Exit code basado en resultados
        if stats["chaotic"] > 0:
            return 1
        elif stats["borderline"] > 0:
            return 2
        return 0

    # Modo single file
    if len(files) == 1:
        path = Path(files[0])
        if not path.exists():
            print(f"ERROR: No encontrado: {path}")
            return 1

        # Validar directorio
        if path.is_dir():
            results = validate_directory(path)
            if results:
                stats = print_detailed_summary(results)
                if stats["chaotic"] > 0:
                    return 1
                elif stats["borderline"] > 0:
                    return 2
            return 0

        # Validar archivo Ãºnico
        result = validate_experiment(path)
        print_result(result)

        # Opciones
        if "--report" in options:
            report = generate_report(result)
            report_path = path.parent / f"{path.stem}_report.txt"
            with open(report_path, 'w') as f:
                f.write(report)
            print(f"\nâœ“ Informe guardado: {report_path}")

        if "--mark" in options:
            if result["unified_status"] == "VALIDADO":
                new_path = mark_experiment(path, result)
                print(f"\nâœ“ Experimento marcado: {new_path}")
            else:
                print(f"\nâœ— No se puede marcar: estado es {result['unified_status']}")

        # Exit code
        status = result["unified_status"]
        return {"VALIDADO": 0, "BORDERLINE": 2, "CAÃ“TICO": 1}.get(status, 1)

    return 0


if __name__ == "__main__":
    sys.exit(main())
